{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritize gene information obtained from previous scripts\n",
    "\n",
    "This priorization filters out genes with corresponding Wikipedia articles that are greater than 10,000 characters in length. It also filters out genes with fewer than 30 publications in PubMed as 5-10 of these articles will likely be more general bioinformatics articles, leaving too few to do a suitable review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import requests\n",
    "\n",
    "datapath = 'data/'\n",
    "resultpath = 'results/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pub_summary(datapath):\n",
    "    pub_details = read_csv(os.path.join(datapath,'PublicationDetailsDF.tsv'), delimiter='\\t',index_col=0,header=0)\n",
    "    pub_details['year'] = pub_details['PublicationDate'].str.extract(r'(\\d\\d\\d\\d)')\n",
    "    pub_details.drop('PubDateType',axis=1,inplace=True)\n",
    "    pub_details.drop_duplicates(keep='first',inplace=True)\n",
    "    pub_details['year'] = pub_details['year'].fillna(0).astype(int)\n",
    "    pub_frequency = pub_details.groupby('geneid').size().reset_index(name='pubcount')\n",
    "    median_year = pub_details.groupby('geneid')['year'].median().reset_index(name='median_pub_year')\n",
    "    median_year['median_pub_year'] = median_year['median_pub_year'].astype(int)\n",
    "    max_year = pub_details.groupby('geneid')['year'].max().reset_index(name='max_pub_year')\n",
    "    max_year['max_pub_year'] = max_year['max_pub_year'].astype(int)\n",
    "    pub_sum = pub_frequency.merge(median_year.merge(max_year,on='geneid',how='left'),on='geneid',how='left').copy()\n",
    "    return(pub_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### page_lengths below 200 are usually page redirects\n",
    "\n",
    "def merge_and_filter_results(datapath,resultpath,min_pubcount=30,min_pagelength=200):\n",
    "    priority_by_size = read_csv(os.path.join(resultpath,'priority_by_size.tsv'), delimiter='\\t',index_col=0,header=0)\n",
    "    priority_by_size.rename(columns={'geneID':'geneid'},inplace=True)\n",
    "    pub_sum = generate_pub_summary(datapath)\n",
    "    gene_summary = priority_by_size.merge(pub_sum,on='geneid',how='inner')\n",
    "    filtered_gene_summary = gene_summary.loc[((gene_summary['pubcount']>min_pubcount) & (gene_summary['page_length']>min_pagelength))].copy()\n",
    "    filtered_gene_summary.sort_values('page_length',ascending=True,inplace=True)\n",
    "    filtered_gene_summary.head(n=500).to_csv(os.path.join(resultpath,'genes_by_wiki_length.tsv'),sep='\\t',header=True)\n",
    "    scored_gene_summary = filtered_gene_summary.copy()\n",
    "    scored_gene_summary['priority_score']= 10000/scored_gene_summary['page_length']+(scored_gene_summary['pubcount']/2800)\n",
    "    scored_gene_summary.sort_values('priority_score',ascending=False,inplace=True)\n",
    "    scored_gene_summary.head(n=500).to_csv(os.path.join(resultpath,'genes_by_score.tsv'),sep='\\t',header=True)\n",
    "    scored_gene_summary.sort_values('pubcount',ascending=False,inplace=True)\n",
    "    scored_gene_summary.head(n=500).to_csv(os.path.join(resultpath,'genes_by_pubcount.tsv'),sep='\\t',header=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_and_filter_results(datapath,resultpath,30,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_no_wiki = read_csv(os.path.join(resultpath,'genes_with_no_gene_protein_wiki.tsv'), delimiter='\\t',index_col=0,header=0)\n",
    "genes_no_wiki.rename(columns={'geneID':'geneid'},inplace=True)\n",
    "print(genes_no_wiki.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_authordf(datapath):\n",
    "    import lzma\n",
    "    import pickle\n",
    "    with lzma.open(os.path.join(datapath,\"author_df.xz\")) as f:\n",
    "        authorpickle = f.read()\n",
    "    authordf = pickle.loads(authorpickle)\n",
    "    return(authordf)\n",
    "\n",
    "def get_clean_authors(datapath):\n",
    "    author_df = load_authordf(datapath)\n",
    "    author_sum = author_df.groupby('AU').size().reset_index(name='count')\n",
    "    no_single_authors = author_sum.loc[author_sum['count']>1].copy()\n",
    "    less_details = author_df.drop(['AuthorDetails','publish_date'],axis=1).copy()\n",
    "    clean_authors = less_details.loc[less_details['AU'].isin(no_single_authors['AU'].tolist())]\n",
    "    return(clean_authors)\n",
    "\n",
    "def get_gene_pmid_table(datapath):\n",
    "    pub_details = read_csv(os.path.join(datapath,'PublicationDetailsDF.tsv'), delimiter='\\t',index_col=0,header=0)\n",
    "    gene_pmid = pub_details[['geneid','pmid']].copy()\n",
    "    gene_pmid.drop_duplicates(keep='first',inplace=True)\n",
    "    return(gene_pmid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_author_table(datapath,resultpath):\n",
    "    author_by_gene = pd.DataFrame(columns=['geneid','AU','counts','FullName','email'])\n",
    "    gene_pmid = get_gene_pmid_table(datapath)\n",
    "    clean_authors = get_clean_authors(datapath)\n",
    "    for eachgene in gene_pmid['geneid'].unique().tolist():\n",
    "        pmids = gene_pmid['pmid'].loc[gene_pmid['geneid']==eachgene]\n",
    "        tmpauths = clean_authors.loc[clean_authors['pmid'].isin(pmids)]\n",
    "        cleanauths = tmpauths.drop_duplicates(subset=['AU','FullName','pmid'],keep='first')\n",
    "        authorlist = cleanauths.groupby(['AU']).size().reset_index(name='counts')\n",
    "        to_keep = authorlist.loc[authorlist['counts']>2].copy()\n",
    "        if len(to_keep)>0:\n",
    "            auth_deets = to_keep.merge(tmpauths,on='AU',how='inner')\n",
    "            auth_deets.drop('pmid',axis=1,inplace=True)\n",
    "            auth_deets.drop_duplicates(keep='first',inplace=True)\n",
    "            auth_deets['geneid']=eachgene\n",
    "            auth_deets.sort_values('counts',ascending=False,inplace=True)\n",
    "            author_by_gene = pd.concat((author_by_gene,auth_deets),ignore_index=True)\n",
    "    author_by_gene.to_csv(os.path.join(resultpath,'potential_authors.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "generate_author_table(datapath,resultpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
