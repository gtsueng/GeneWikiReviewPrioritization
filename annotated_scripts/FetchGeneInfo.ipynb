{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling and prioritizing Genes for Wikipedia updates\n",
    "\n",
    "**Pulling Gene/Protein information**\n",
    "This code uses SPARL queries to pull the Wikipedia page links for all human genes and proteins from Wikidata. It then pulls the page length of each gene/protein page in Wikipedia. Next, it pushes the genes to NCBI using the biopython library to determine the number of PMIDS associated with each gene.\n",
    "\n",
    "**Prioritizing Gene/Protein information**\n",
    "The wikipedia pagelengths are merged with the PMID publication records and filtered to remove genes with Wikipedia articles that are >10,000 characters and genes with <30 publications in PubMed. High priority genes are those with >70 articles in PubMed, but are <2000 characters\n",
    "\n",
    "**Identifying potential authors for invitation to the Gene Wiki Review Series**\n",
    "Once a list of high priority genes are identified, the genes are converted to entrez gene ids and the pubmed records are used to identify authors. The authors for each publication is aggregated for each gene and the authors are sorted from highest number of publications to least.\n",
    "\n",
    "**Directory structure**\n",
    "Data pulled directly from Wikidata or elsewhere is saved to the 'data/' path. Attempts at sorting, filtering, or prioritizing information from the data will be saved to the 'results/' path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Import libraries\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "import time\n",
    "import mwclient as mw\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Request nicely\n",
    "###############################################################################\n",
    "\n",
    "DEFAULT_TIMEOUT = 5 # seconds\n",
    "\n",
    "class TimeoutHTTPAdapter(HTTPAdapter):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.timeout = DEFAULT_TIMEOUT\n",
    "        if \"timeout\" in kwargs:\n",
    "            self.timeout = kwargs[\"timeout\"]\n",
    "            del kwargs[\"timeout\"]\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def send(self, request, **kwargs):\n",
    "        timeout = kwargs.get(\"timeout\")\n",
    "        if timeout is None:\n",
    "            kwargs[\"timeout\"] = self.timeout\n",
    "        return super().send(request, **kwargs)\n",
    "\n",
    "## Set time outs, backoff, retries\n",
    "httprequests = requests.Session()\n",
    "retry_strategy = Retry(\n",
    "    total=3,\n",
    "    backoff_factor=1,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"] ## Note this method is deprecated and replaced with `allowed_methods` for newer releases of requests library\n",
    "    #allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"] ## Note this method is deprecated and replaced with `allowed_methods` for newer releases of requests library\n",
    ")\n",
    "adapter = TimeoutHTTPAdapter(timeout=25,max_retries=retry_strategy)\n",
    "httprequests.mount(\"https://\", adapter)\n",
    "httprequests.mount(\"http://\", adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## This module uses mwclient to pull page size and edit stats on wikipedia pages  \n",
    "## for each gene given a list of gene wikipedia titles\n",
    "###############################################################################\n",
    "def get_wiki_volume_info (mwsite,titlelist):\n",
    "    print('obtaining wikipedia volume information')\n",
    "    titlelist\n",
    "    pageinfo=[]\n",
    "    pagefails = []\n",
    "    for eachpage in titlelist:\n",
    "        tempdict={} #title, length/size, last_revised, last_revision_id\n",
    "        try:\n",
    "            checkitem = mwsite.api('query', prop='info', titles=eachpage)\n",
    "            results1 = checkitem['query']['pages']\n",
    "            for item in results1:\n",
    "                base = str(item)\n",
    "                results2 = results1[base]\n",
    "                tempdict['title']=str(results2['title'])\n",
    "                tempdict['page_length']=int(results2['length'])\n",
    "                tempdict['last_touched']=str(results2['touched'])\n",
    "                tempdict['lastrevid']=str(results2['lastrevid'])\n",
    "                pageinfo.append(tempdict)               \n",
    "        except:\n",
    "            pagefails.append(eachpage)\n",
    "            pass \n",
    "        time.sleep(1)\n",
    "    return(pageinfo,pagefails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## This module uses pulls pageview data from the Media Wiki PageViews API\n",
    "## More on the API here: https://wikimedia.org/api/rest_v1/#/Pageviews%20data/\n",
    "## The module pulls in a parameter dictionary, and the list of wiki titles\n",
    "## Parameters include:\n",
    "## project: en.wikipedia.org, other wikimedia projects\n",
    "## access: all-access, desktop, mobile-app, mobile-web\n",
    "## agent: all-agents, user, spider, bot\n",
    "## granularity: daily, monthly\n",
    "###############################################################################\n",
    "def get_monthly_pvs(page_view_parameters, useragent, no_missing):\n",
    "    no_missing['titlelist'] = [x.replace(\" \",\"_\").replace(\"https://\",\"http://\").replace(\"http://en.wikipedia.org/wiki/\",\"\") for x in no_missing['Gene Wiki Page']]\n",
    "    pginfo = []\n",
    "    pgfails = []\n",
    "    print('obtaining wikipedia pageview information')\n",
    "    pv_api_url = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/\"\n",
    "    for eachtitle in no_missing['titlelist']:\n",
    "        try:\n",
    "            url = pv_api_url+pv_params['access']+pv_params['agent']+eachtitle+\"/\"+pv_params['granularity']+pv_params['start']+\"/\"+pv_params['end']\n",
    "            r = httprequests.get(url, headers=useragent)\n",
    "            items = r.json()\n",
    "            try:\n",
    "                for item in items[\"items\"]:\n",
    "                    tmpdict = {'title':item[\"article\"], 'views':int(item[\"views\"]), 'granularity':item['granularity'],\n",
    "                               'timestamp':item[\"timestamp\"],'access':item['access'],'agent':item['agent']}\n",
    "                    pginfo.append(tmpdict)\n",
    "            except:\n",
    "                tmpdict = {'title':title, 'views':-1, 'granularity':\"no data\",\n",
    "                               'timestamp':\"00000000\",'access':\"not data\",'agent':\"no data\"}\n",
    "                pginfo.append(tmpdict)            \n",
    "        except:\n",
    "            pgfails.append(eachtitle)\n",
    "        time.sleep(1)\n",
    "\n",
    "    pginfodf = pd.DataFrame(pginfo)\n",
    "    \n",
    "    return(pginfodf, pgfails)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Wikidata and save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generate table of all human genes that have do not have a Wikipedia article\n",
    "def get_genes_no_wiki(datapath):\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    query = \"\"\"\n",
    "        SELECT ?item ?itemLabel ?geneID ?proteinwdid\n",
    "        WHERE\n",
    "        {\n",
    "          ?item wdt:P31 wd:Q7187 .\n",
    "          ?item wdt:P703 wd:Q15978631 .\n",
    "          ?item wdt:P351 ?geneID .\n",
    "          ?item wdt:P688 ?proteinwdid .\n",
    "          ?sitelink schema:about ?item .\n",
    "          FILTER NOT EXISTS {\n",
    "            ?article schema:about ?item .\n",
    "            ?article schema:isPartOf <https://en.wikipedia.org/> .       }\n",
    "\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" }\n",
    "        }\n",
    "    \"\"\"\n",
    "    r = httprequests.get(url, params = {'format': 'json', 'query': query})\n",
    "    data = r.json()\n",
    "    datadf = pd.DataFrame(data['results']['bindings'])\n",
    "    datadf['uri'] = [x['value'] for x in datadf['item']]\n",
    "    datadf['label'] = [x['value'] for x in datadf['itemLabel']]\n",
    "    datadf['geneID'] = [x['value'] for x in datadf['geneID']]\n",
    "    datadf['QID'] = [x.replace('http://www.wikidata.org/entity/','') for x in datadf['uri']]\n",
    "    datadf['proteinuri'] = [x['value'] for x in datadf['proteinwdid']]\n",
    "    datadf['proteinID'] = [x.replace('http://www.wikidata.org/entity/','') for x in datadf['proteinuri']]\n",
    "    cleandata = datadf[['QID','label','geneID','proteinID']].copy()\n",
    "    cleandata.drop_duplicates(keep='first', inplace=True)\n",
    "    cleandata.to_csv(os.path.join(datapath,'genes_no_wiki.tsv'),sep = '\\t', header=True)\n",
    "    print(len(cleandata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generate table of all human genes, where the protein encoded by those genes do not have a Wikipedia article\n",
    "def get_gene_proteins_no_wiki(datapath):\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    query = \"\"\"\n",
    "        SELECT ?item ?itemLabel ?geneID ?proteinwdid\n",
    "        WHERE\n",
    "        {\n",
    "          ?item wdt:P31 wd:Q7187 .\n",
    "          ?item wdt:P703 wd:Q15978631 .\n",
    "          ?item wdt:P351 ?geneID .\n",
    "          ?item wdt:P688 ?proteinwdid .\n",
    "          ?sitelink schema:about ?item .\n",
    "          FILTER NOT EXISTS {\n",
    "            ?article schema:about ?proteinwdid .\n",
    "            ?article schema:isPartOf <https://en.wikipedia.org/> .       }\n",
    "\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" }\n",
    "        }\n",
    "    \"\"\"\n",
    "    r = httprequests.get(url, params = {'format': 'json', 'query': query})\n",
    "    data = r.json()\n",
    "    datadf = pd.DataFrame(data['results']['bindings'])\n",
    "    datadf['uri'] = [x['value'] for x in datadf['item']]\n",
    "    datadf['label'] = [x['value'] for x in datadf['itemLabel']]\n",
    "    datadf['geneID'] = [x['value'] for x in datadf['geneID']]\n",
    "    datadf['QID'] = [x.replace('http://www.wikidata.org/entity/','') for x in datadf['uri']]\n",
    "    datadf['proteinuri'] = [x['value'] for x in datadf['proteinwdid']]\n",
    "    datadf['proteinID'] = [x.replace('http://www.wikidata.org/entity/','') for x in datadf['proteinuri']]\n",
    "    cleandata = datadf[['QID','label','geneID','proteinID']].copy()\n",
    "    cleandata.drop_duplicates(keep='first', inplace=True)\n",
    "    cleandata.to_csv(os.path.join(datapath,'proteins_no_wiki.tsv'),sep = '\\t', header=True)\n",
    "    print(len(cleandata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generate table of all human genes with a Wikipedia article\n",
    "def get_genes_with_wiki(datapath):\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    query = \"\"\"\n",
    "        SELECT ?item ?itemLabel ?geneID ?proteinwdid ?sitelink\n",
    "        WHERE\n",
    "        {\n",
    "          ?item wdt:P31 wd:Q7187 .\n",
    "          ?item wdt:P703 wd:Q15978631 .\n",
    "          ?item wdt:P351 ?geneID .\n",
    "          ?item wdt:P688 ?proteinwdid .\n",
    "          ?sitelink schema:about ?item .\n",
    "          FILTER EXISTS {\n",
    "            ?article schema:about ?item .\n",
    "            ?article schema:isPartOf <https://en.wikipedia.org/> .\n",
    "          }\n",
    "\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" }\n",
    "        }\n",
    "    \"\"\"\n",
    "    r = httprequests.get(url, params = {'format': 'json', 'query': query})\n",
    "    data = r.json()\n",
    "    datadf = pd.DataFrame(data['results']['bindings'])\n",
    "    datadf['uri'] = [x['value'] for x in datadf['item']]\n",
    "    datadf['label'] = [x['value'] for x in datadf['itemLabel']]\n",
    "    datadf['geneID'] = [x['value'] for x in datadf['geneID']]\n",
    "    datadf['QID'] = [x.replace('http://www.wikidata.org/entity/','') for x in datadf['uri']]\n",
    "    datadf['proteinuri'] = [x['value'] for x in datadf['proteinwdid']]\n",
    "    datadf['proteinID'] = [x.replace('http://www.wikidata.org/entity/','') for x in datadf['proteinuri']]\n",
    "    datadf['wikilink'] = [x['value'] for x in datadf['sitelink']]\n",
    "    cleandata = datadf[['QID','label','geneID','proteinID','wikilink']].copy()\n",
    "    en_only = cleandata.loc[cleandata['wikilink'].str.contains('en.wiki')].copy()\n",
    "    en_only.drop_duplicates(keep='first',inplace=True)\n",
    "    en_only.to_csv(os.path.join(datapath,'genes_en_wiki.tsv'),sep = '\\t', header=True)\n",
    "    print(len(en_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generate table of all human genes, where the protein encoded by those genes does have a Wikipedia article\n",
    "def get_genes_proteins_with_wiki(datapath):\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    query = \"\"\"\n",
    "        SELECT ?item ?itemLabel ?geneID ?proteinwdid ?sitelink\n",
    "        WHERE\n",
    "        {\n",
    "          ?item wdt:P31 wd:Q7187 .\n",
    "          ?item wdt:P703 wd:Q15978631 .\n",
    "          ?item wdt:P351 ?geneID .\n",
    "          ?item wdt:P688 ?proteinwdid .\n",
    "          ?sitelink schema:about ?proteinwdid .\n",
    "          FILTER EXISTS {\n",
    "            ?article schema:about ?proteinwdid .\n",
    "            ?article schema:isPartOf <https://en.wikipedia.org/> .\n",
    "          }\n",
    "\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" }\n",
    "        }\n",
    "    \"\"\"\n",
    "    r = httprequests.get(url, params = {'format': 'json', 'query': query})\n",
    "    data = r.json()\n",
    "    datadf = pd.DataFrame(data['results']['bindings'])\n",
    "    datadf['uri'] = [x['value'] for x in datadf['item']]\n",
    "    datadf['label'] = [x['value'] for x in datadf['itemLabel']]\n",
    "    datadf['geneID'] = [x['value'] for x in datadf['geneID']]\n",
    "    datadf['QID'] = [x.replace('http://www.wikidata.org/entity/','') for x in datadf['uri']]\n",
    "    datadf['proteinuri'] = [x['value'] for x in datadf['proteinwdid']]\n",
    "    datadf['proteinID'] = [x.replace('http://www.wikidata.org/entity/','') for x in datadf['proteinuri']]\n",
    "    datadf['wikilink'] = [x['value'] for x in datadf['sitelink']]\n",
    "    cleandata = datadf[['QID','label','geneID','proteinID','wikilink']].copy()\n",
    "    en_only = cleandata.loc[cleandata['wikilink'].str.contains('en.wiki')].copy()\n",
    "    en_only.drop_duplicates(keep='first',inplace=True)\n",
    "    en_only.to_csv(os.path.join(datapath,'proteins_en_wiki.tsv'),sep = '\\t', header=True)\n",
    "    print(len(en_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wd_info(datapath):\n",
    "    print(\"fetching info from Wikidata: \",datetime.now())\n",
    "    get_genes_no_wiki(datapath)\n",
    "    get_gene_proteins_no_wiki(datapath)\n",
    "    get_genes_with_wiki(datapath)\n",
    "    get_genes_proteins_with_wiki(datapath)\n",
    "    print(\"fetching complete: \",datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the results for filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Merge table of genes and proteins to identify Genes which do NOT have wikipedia articles, \n",
    "#### which encode proteins that do NOT have Wikipedia articles\n",
    "#### ie - Identify genes which show up on both lists (no gene article, no protein article)\n",
    "def filter_no_wikis(datapath,resultpath):\n",
    "    genes_no_wiki = pd.read_csv(os.path.join(datapath,'genes_no_wiki.tsv'),delimiter = '\\t', header=0, index_col=0)\n",
    "    proteins_no_wiki = pd.read_csv(os.path.join(datapath,'proteins_no_wiki.tsv'),delimiter = '\\t', header=0, index_col=0)\n",
    "    no_wiki_merge = pd.concat((genes_no_wiki,proteins_no_wiki),ignore_index=True)\n",
    "    frequency = no_wiki_merge.groupby('geneID').size().reset_index(name='counts')\n",
    "    no_gene_protein = frequency.loc[frequency['counts']==2]\n",
    "    no_gene_protein_info = genes_no_wiki.loc[genes_no_wiki['geneID'].isin(no_gene_protein['geneID'].tolist())]\n",
    "    no_gene_protein_info.to_csv(os.path.join(resultpath,'genes_with_no_gene_protein_wiki.tsv'),sep='\\t',header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Merge table of genes and proteins to identify Genes which have a wikipedia article or\n",
    "#### the protein encoded by those genes has a Wikipedia article\n",
    "#### ie - Identify all unique wikilinks and their corresponding gene --  this is the merged list\n",
    "#### Then pull page length for all Wikipedia articles and merged list\n",
    "#### Filter out Wikipedia articles which are greater than 10,000 characters in length\n",
    "\n",
    "def filter_wikis(datapath,resultpath):\n",
    "    genes_en_wiki = pd.read_csv(os.path.join(datapath,'genes_en_wiki.tsv'),delimiter = '\\t', header=0, index_col=0)\n",
    "    proteins_en_wiki = pd.read_csv(os.path.join(datapath,'proteins_en_wiki.tsv'),delimiter = '\\t', header=0, index_col=0)\n",
    "    en_wiki_merge = pd.concat((genes_en_wiki,proteins_en_wiki),ignore_index=True)\n",
    "    unique_wikis = en_wiki_merge.groupby(['geneID','proteinID','wikilink']).size().reset_index(name='counts')\n",
    "    unique_wikis.to_csv(os.path.join(datapath,'gene_protein_wikilinks.tsv'),sep='\\t',header=True)\n",
    "    unique_wikis = pd.read_csv(os.path.join(datapath,'gene_protein_wikilinks.tsv'),delimiter='\\t',header=0,index_col=0)\n",
    "    unique_wikis['title'] = [x.replace(\" \",\"_\").replace(\"https://\",\"http://\").replace(\"http://en.wikipedia.org/wiki/\",\"\") for x in unique_wikis['wikilink']]\n",
    "    titlelist = unique_wikis['title'].unique().tolist()\n",
    "    pageinfo,pagefails = get_wiki_volume_info(mwsite,titlelist)\n",
    "    wikiinfo = pd.DataFrame(pageinfo)\n",
    "    wikiinfo.to_csv(os.path.join(datapath,'gene_wiki_vol_info.tsv'),sep='\\t',header=True)\n",
    "    shorter_articles = wikiinfo.loc[wikiinfo['page_length']<10000].copy()\n",
    "    shorter_articles.sort_values('page_length',ascending=True,inplace=True)\n",
    "    detailed_shorter_articles = shorter_articles.merge(unique_wikis,on='title',how='inner')\n",
    "    detailed_shorter_articles.to_csv(os.path.join(resultpath,'priority_by_size.tsv'),sep='\\t',header=True)\n",
    "    print(len(detailed_shorter_articles))\n",
    "    print(detailed_shorter_articles.head(n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script started:  2022-03-23 10:22:57.815982\n",
      "fetching info from Wikidata:  2022-03-23 10:22:57.820673\n",
      "8261\n",
      "22957\n",
      "15041\n",
      "1187\n",
      "fetching complete:  2022-03-23 10:25:22.299208\n",
      "wdinfo_script_complete:  2022-03-23 10:25:22.299726\n",
      "genes/proteins, no wiki, filtered:  2022-03-23 10:25:22.381656\n",
      "obtaining wikipedia volume information\n",
      "9259\n",
      "        title  page_length          last_touched  lastrevid  geneID  \\\n",
      "0     FAM234A           19  2021-09-27T20:11:03Z  848630202   83986   \n",
      "1  Granulysin           35  2022-02-14T06:10:27Z  902999968   10578   \n",
      "\n",
      "   proteinID                                  wikilink  counts  \n",
      "0  Q21437672     https://en.wikipedia.org/wiki/FAM234A       1  \n",
      "1   Q5596842  https://en.wikipedia.org/wiki/Granulysin       1  \n",
      "scripts completelyl run:  2022-03-23 14:46:52.634018\n",
      "Wall time: 4h 23min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### Paths\n",
    "datapath = 'data/'\n",
    "resultpath = 'results/'\n",
    "\n",
    "#### Config\n",
    "#useragent = {\n",
    "#    'User-Agent': 'Gene Wiki Review Impact (youremail@domain)'\n",
    "#}\n",
    "useragent = os.environ['USER_AGENT']\n",
    "\n",
    "mwsite = mw.Site('en.wikipedia.org', clients_useragent=useragent['User-Agent'])\n",
    "\n",
    "print(\"script started: \",datetime.now())\n",
    "get_wd_info(datapath)\n",
    "print(\"wdinfo_script_complete: \",datetime.now())\n",
    "filter_no_wikis(datapath,resultpath)\n",
    "print(\"genes/proteins, no wiki, filtered: \",datetime.now())\n",
    "filter_wikis(datapath,resultpath)\n",
    "print(\"scripts completely run: \",datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
