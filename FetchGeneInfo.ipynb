{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling and prioritizing Genes for Wikipedia updates\n",
    "\n",
    "**Pulling Gene/Protein information**\n",
    "This code uses SPARL queries to pull the Wikipedia page links for all human genes and proteins from Wikidata. It then pulls the page length of each gene/protein page in Wikipedia. Next, it pushes the genes to NCBI using the biopython library to determine the number of PMIDS associated with each gene.\n",
    "\n",
    "**Prioritizing Gene/Protein information**\n",
    "The wikipedia pagelengths are merged with the PMID publication records and filtered to remove genes with Wikipedia articles that are >10,000 characters and genes with <30 publications in PubMed. High priority genes are those with >70 articles in PubMed, but are <2000 characters\n",
    "\n",
    "**Identifying potential authors for invitation to the Gene Wiki Review Series**\n",
    "Once a list of high priority genes are identified, the genes are converted to entrez gene ids and the pubmed records are used to identify authors. The authors for each publication is aggregated for each gene and the authors are sorted from highest number of publications to least."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Import libraries\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "import time\n",
    "import mwclient as mw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Paths\n",
    "\n",
    "datapath = 'data/'\n",
    "\n",
    "#### Config\n",
    "useragent = {\n",
    "    'User-Agent': 'Gene Wiki Review Impact (youremail@domain)'\n",
    "}\n",
    "\n",
    "mwsite = mw.Site('en.wikipedia.org', clients_useragent=useragent['User-Agent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Request nicely\n",
    "###############################################################################\n",
    "\n",
    "DEFAULT_TIMEOUT = 5 # seconds\n",
    "\n",
    "class TimeoutHTTPAdapter(HTTPAdapter):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.timeout = DEFAULT_TIMEOUT\n",
    "        if \"timeout\" in kwargs:\n",
    "            self.timeout = kwargs[\"timeout\"]\n",
    "            del kwargs[\"timeout\"]\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def send(self, request, **kwargs):\n",
    "        timeout = kwargs.get(\"timeout\")\n",
    "        if timeout is None:\n",
    "            kwargs[\"timeout\"] = self.timeout\n",
    "        return super().send(request, **kwargs)\n",
    "\n",
    "## Set time outs, backoff, retries\n",
    "httprequests = requests.Session()\n",
    "retry_strategy = Retry(\n",
    "    total=3,\n",
    "    backoff_factor=1,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"] ## Note this method is deprecated and replaced with `allowed_methods` for newer releases of requests library\n",
    "    #allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"] ## Note this method is deprecated and replaced with `allowed_methods` for newer releases of requests library\n",
    ")\n",
    "adapter = TimeoutHTTPAdapter(timeout=5,max_retries=retry_strategy)\n",
    "httprequests.mount(\"https://\", adapter)\n",
    "httprequests.mount(\"http://\", adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## This module uses mwclient to pull page size and edit stats on wikipedia pages  \n",
    "## for each gene given a list of gene wikipedia titles\n",
    "###############################################################################\n",
    "def get_wiki_volume_info (mwsite,titlelist):\n",
    "    print('obtaining wikipedia volume information')\n",
    "    titlelist\n",
    "    pageinfo=[]\n",
    "    pagefails = []\n",
    "    for eachpage in titlelist:\n",
    "        tempdict={} #title, length/size, last_revised, last_revision_id\n",
    "        try:\n",
    "            checkitem = mwsite.api('query', prop='info', titles=eachpage)\n",
    "            results1 = checkitem['query']['pages']\n",
    "            for item in results1:\n",
    "                base = str(item)\n",
    "                results2 = results1[base]\n",
    "                tempdict['title']=str(results2['title'])\n",
    "                tempdict['page_length']=int(results2['length'])\n",
    "                tempdict['last_touched']=str(results2['touched'])\n",
    "                tempdict['lastrevid']=str(results2['lastrevid'])\n",
    "                pageinfo.append(tempdict)               \n",
    "        except:\n",
    "            pagefails.append(eachpage)\n",
    "            pass \n",
    "        time.sleep(1)\n",
    "    return(pageinfo,pagefails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## This module uses pulls pageview data from the Media Wiki PageViews API\n",
    "## More on the API here: https://wikimedia.org/api/rest_v1/#/Pageviews%20data/\n",
    "## The module pulls in a parameter dictionary, and the list of wiki titles\n",
    "## Parameters include:\n",
    "## project: en.wikipedia.org, other wikimedia projects\n",
    "## access: all-access, desktop, mobile-app, mobile-web\n",
    "## agent: all-agents, user, spider, bot\n",
    "## granularity: daily, monthly\n",
    "###############################################################################\n",
    "def get_monthly_pvs(page_view_parameters, useragent, no_missing):\n",
    "    no_missing['titlelist'] = [x.replace(\" \",\"_\").replace(\"https://\",\"http://\").replace(\"http://en.wikipedia.org/wiki/\",\"\") for x in no_missing['Gene Wiki Page']]\n",
    "    pginfo = []\n",
    "    pgfails = []\n",
    "    print('obtaining wikipedia pageview information')\n",
    "    pv_api_url = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/\"\n",
    "    for eachtitle in no_missing['titlelist']:\n",
    "        try:\n",
    "            url = pv_api_url+pv_params['access']+pv_params['agent']+eachtitle+\"/\"+pv_params['granularity']+pv_params['start']+\"/\"+pv_params['end']\n",
    "            r = httprequests.get(url, headers=useragent)\n",
    "            items = r.json()\n",
    "            try:\n",
    "                for item in items[\"items\"]:\n",
    "                    tmpdict = {'title':item[\"article\"], 'views':int(item[\"views\"]), 'granularity':item['granularity'],\n",
    "                               'timestamp':item[\"timestamp\"],'access':item['access'],'agent':item['agent']}\n",
    "                    pginfo.append(tmpdict)\n",
    "            except:\n",
    "                tmpdict = {'title':title, 'views':-1, 'granularity':\"no data\",\n",
    "                               'timestamp':\"00000000\",'access':\"not data\",'agent':\"no data\"}\n",
    "                pginfo.append(tmpdict)            \n",
    "        except:\n",
    "            pgfails.append(eachtitle)\n",
    "        time.sleep(1)\n",
    "\n",
    "    pginfodf = pd.DataFrame(pginfo)\n",
    "    \n",
    "    return(pginfodf, pgfails)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Wikidata and save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8250\n"
     ]
    }
   ],
   "source": [
    "#### Generate table of all human genes that have do not have a Wikipedia article\n",
    "url = 'https://query.wikidata.org/sparql'\n",
    "query = \"\"\"\n",
    "    SELECT ?item ?itemLabel ?geneID ?proteinwdid\n",
    "    WHERE\n",
    "    {\n",
    "      ?item wdt:P31 wd:Q7187 .\n",
    "      ?item wdt:P703 wd:Q15978631 .\n",
    "      ?item wdt:P351 ?geneID .\n",
    "      ?item wdt:P688 ?proteinwdid .\n",
    "      ?sitelink schema:about ?item .\n",
    "      FILTER NOT EXISTS {\n",
    "        ?article schema:about ?item .\n",
    "        ?article schema:isPartOf <https://en.wikipedia.org/> .       }\n",
    "\n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" }\n",
    "    }\n",
    "\"\"\"\n",
    "r = httprequests.get(url, params = {'format': 'json', 'query': query})\n",
    "data = r.json()\n",
    "datadf = pd.DataFrame(data['results']['bindings'])\n",
    "datadf['uri'] = [x['value'] for x in datadf['item']]\n",
    "datadf['label'] = [x['value'] for x in datadf['itemLabel']]\n",
    "datadf['geneID'] = [x['value'] for x in datadf['geneID']]\n",
    "datadf['QID'] = [x.replace('http://www.wikidata.org/entity/','') for x in datadf['uri']]\n",
    "datadf['proteinuri'] = [x['value'] for x in datadf['proteinwdid']]\n",
    "datadf['proteinID'] = [x.replace('http://www.wikidata.org/entity/','') for x in datadf['proteinuri']]\n",
    "cleandata = datadf[['QID','label','geneID','proteinID']].copy()\n",
    "cleandata.drop_duplicates(keep='first', inplace=True)\n",
    "cleandata.to_csv(os.path.join(datapath,'genes_no_wiki.tsv'),sep = '\\t', header=True)\n",
    "print(len(cleandata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generate table of all human genes, where the protein encoded by those genes do not have a Wikipedia article\n",
    "url = 'https://query.wikidata.org/sparql'\n",
    "query = \"\"\"\n",
    "    SELECT ?item ?itemLabel ?geneID ?proteinwdid\n",
    "    WHERE\n",
    "    {\n",
    "      ?item wdt:P31 wd:Q7187 .\n",
    "      ?item wdt:P703 wd:Q15978631 .\n",
    "      ?item wdt:P351 ?geneID .\n",
    "      ?item wdt:P688 ?proteinwdid .\n",
    "      ?sitelink schema:about ?item .\n",
    "      FILTER NOT EXISTS {\n",
    "        ?article schema:about ?proteinwdid .\n",
    "        ?article schema:isPartOf <https://en.wikipedia.org/> .       }\n",
    "\n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" }\n",
    "    }\n",
    "\"\"\"\n",
    "r = httprequests.get(url, params = {'format': 'json', 'query': query})\n",
    "data = r.json()\n",
    "datadf = pd.DataFrame(data['results']['bindings'])\n",
    "datadf['uri'] = [x['value'] for x in datadf['item']]\n",
    "datadf['label'] = [x['value'] for x in datadf['itemLabel']]\n",
    "datadf['geneID'] = [x['value'] for x in datadf['geneID']]\n",
    "datadf['QID'] = [x.replace('http://www.wikidata.org/entity/','') for x in datadf['uri']]\n",
    "datadf['proteinuri'] = [x['value'] for x in datadf['proteinwdid']]\n",
    "datadf['proteinID'] = [x.replace('http://www.wikidata.org/entity/','') for x in datadf['proteinuri']]\n",
    "cleandata = datadf[['QID','label','geneID','proteinID']].copy()\n",
    "cleandata.drop_duplicates(keep='first', inplace=True)\n",
    "cleandata.to_csv(os.path.join(datapath,'proteins_no_wiki.tsv'),sep = '\\t', header=True)\n",
    "print(len(cleandata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generate table of all human genes with a Wikipedia article\n",
    "\n",
    "url = 'https://query.wikidata.org/sparql'\n",
    "query = \"\"\"\n",
    "    SELECT ?item ?itemLabel ?geneID ?proteinwdid ?sitelink\n",
    "    WHERE\n",
    "    {\n",
    "      ?item wdt:P31 wd:Q7187 .\n",
    "      ?item wdt:P703 wd:Q15978631 .\n",
    "      ?item wdt:P351 ?geneID .\n",
    "      ?item wdt:P688 ?proteinwdid .\n",
    "      ?sitelink schema:about ?item .\n",
    "      FILTER EXISTS {\n",
    "        ?article schema:about ?item .\n",
    "        ?article schema:isPartOf <https://en.wikipedia.org/> .\n",
    "      }\n",
    "\n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" }\n",
    "    }\n",
    "\"\"\"\n",
    "r = httprequests.get(url, params = {'format': 'json', 'query': query})\n",
    "data = r.json()\n",
    "datadf = pd.DataFrame(data['results']['bindings'])\n",
    "datadf['uri'] = [x['value'] for x in datadf['item']]\n",
    "datadf['label'] = [x['value'] for x in datadf['itemLabel']]\n",
    "datadf['geneID'] = [x['value'] for x in datadf['geneID']]\n",
    "datadf['QID'] = [x.replace('http://www.wikidata.org/entity/','') for x in datadf['uri']]\n",
    "datadf['proteinuri'] = [x['value'] for x in datadf['proteinwdid']]\n",
    "datadf['proteinID'] = [x.replace('http://www.wikidata.org/entity/','') for x in datadf['proteinuri']]\n",
    "datadf['wikilink'] = [x['value'] for x in datadf['sitelink']]\n",
    "cleandata = datadf[['QID','label','geneID','proteinID','wikilink']].copy()\n",
    "en_only = cleandata.loc[cleandata['wikilink'].str.contains('en.wiki')].copy()\n",
    "en_only.drop_duplicates(keep='first',inplace=True)\n",
    "en_only.to_csv(os.path.join(datapath,'genes_en_wiki.tsv'),sep = '\\t', header=True)\n",
    "print(len(en_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generate table of all human genes, where the protein encoded by those genes does have a Wikipedia article\n",
    "\n",
    "url = 'https://query.wikidata.org/sparql'\n",
    "query = \"\"\"\n",
    "    SELECT ?item ?itemLabel ?geneID ?proteinwdid ?sitelink\n",
    "    WHERE\n",
    "    {\n",
    "      ?item wdt:P31 wd:Q7187 .\n",
    "      ?item wdt:P703 wd:Q15978631 .\n",
    "      ?item wdt:P351 ?geneID .\n",
    "      ?item wdt:P688 ?proteinwdid .\n",
    "      ?sitelink schema:about ?proteinwdid .\n",
    "      FILTER EXISTS {\n",
    "        ?article schema:about ?proteinwdid .\n",
    "        ?article schema:isPartOf <https://en.wikipedia.org/> .\n",
    "      }\n",
    "\n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" }\n",
    "    }\n",
    "\"\"\"\n",
    "r = httprequests.get(url, params = {'format': 'json', 'query': query})\n",
    "data = r.json()\n",
    "datadf = pd.DataFrame(data['results']['bindings'])\n",
    "datadf['uri'] = [x['value'] for x in datadf['item']]\n",
    "datadf['label'] = [x['value'] for x in datadf['itemLabel']]\n",
    "datadf['geneID'] = [x['value'] for x in datadf['geneID']]\n",
    "datadf['QID'] = [x.replace('http://www.wikidata.org/entity/','') for x in datadf['uri']]\n",
    "datadf['proteinuri'] = [x['value'] for x in datadf['proteinwdid']]\n",
    "datadf['proteinID'] = [x.replace('http://www.wikidata.org/entity/','') for x in datadf['proteinuri']]\n",
    "datadf['wikilink'] = [x['value'] for x in datadf['sitelink']]\n",
    "cleandata = datadf[['QID','label','geneID','proteinID','wikilink']].copy()\n",
    "en_only = cleandata.loc[cleandata['wikilink'].str.contains('en.wiki')].copy()\n",
    "en_only.drop_duplicates(keep='first',inplace=True)\n",
    "en_only.to_csv(os.path.join(datapath,'proteins_en_wiki.tsv'),sep = '\\t', header=True)\n",
    "print(len(en_only))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the results for filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          QID   label  geneID  proteinID\n",
      "4   Q18026568    GSC2    2928  Q21119009\n",
      "10  Q18026613  GTF2A2    2958  Q21135074\n"
     ]
    }
   ],
   "source": [
    "#### Merge table of genes and proteins to identify Genes which do NOT have wikipedia articles, \n",
    "#### which encode proteins that do NOT have Wikipedia articles\n",
    "genes_no_wiki = pd.read_csv(os.path.join(datapath,'genes_no_wiki.tsv'),delimiter = '\\t', header=0, index_col=0)\n",
    "proteins_no_wiki = pd.read_csv(os.path.join(datapath,'proteins_no_wiki.tsv'),delimiter = '\\t', header=0, index_col=0)\n",
    "#print(genes_no_wiki.head(n=2))\n",
    "#print(proteins_no_wiki.head(n=2))\n",
    "no_wiki_merge = pd.concat((genes_no_wiki,proteins_no_wiki),ignore_index=True)\n",
    "\n",
    "## Identify genes which show up on both lists (no gene article, no protein article)\n",
    "frequency = no_wiki_merge.groupby('geneID').size().reset_index(name='counts')\n",
    "no_gene_protein = frequency.loc[frequency['counts']==2]\n",
    "no_gene_protein_info = genes_no_wiki.loc[genes_no_wiki['geneID'].isin(no_gene_protein['geneID'].tolist())]\n",
    "no_gene_protein_info.to_csv(os.path.join(datapath,'genes_with_no_gene_protein_wiki.tsv'),sep='t',header=True)\n",
    "print(no_gene_protein_info.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          QID label  geneID proteinID  \\\n",
      "18  Q14902024  CTSS    1520  Q4217286   \n",
      "23  Q14902030  CTSO    1519  Q5052488   \n",
      "\n",
      "                                     wikilink  \n",
      "18  https://en.wikipedia.org/wiki/Cathepsin_S  \n",
      "23  https://en.wikipedia.org/wiki/Cathepsin_O  \n",
      "         QID  label  geneID  proteinID                             wikilink\n",
      "3  Q17710375  PARP1     142  Q21112674  https://en.wikipedia.org/wiki/PARP1\n",
      "8  Q18039521   GIT1   28964  Q21112695   https://en.wikipedia.org/wiki/GIT1\n"
     ]
    }
   ],
   "source": [
    "#### Merge table of genes and proteins to identify Genes which have a wikipedia article or\n",
    "#### the protein encoded by those genes has a Wikipedia article\n",
    "genes_en_wiki = pd.read_csv(os.path.join(datapath,'genes_en_wiki.tsv'),delimiter = '\\t', header=0, index_col=0)\n",
    "proteins_en_wiki = pd.read_csv(os.path.join(datapath,'proteins_en_wiki.tsv'),delimiter = '\\t', header=0, index_col=0)\n",
    "print(genes_en_wiki.head(n=2))\n",
    "print(proteins_en_wiki.head(n=2))\n",
    "\n",
    "## Identify all unique wikilinks and their corresponding gene --  this is the merged list\n",
    "en_wiki_merge = pd.concat((genes_en_wiki,proteins_en_wiki),ignore_index=True)\n",
    "unique_wikis = en_wiki_merge.groupby(['geneID','proteinID','wikilink']).size().reset_index(name='counts')\n",
    "unique_wikis.to_csv(os.path.join(datapath,'gene_protein_wikilinks.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Pull page length for all Wikipedia articles merged list\n",
    "unique_wikis = pd.read_csv(os.path.join(datapath,'gene_protein_wikilinks.tsv'),delimiter='\\t',header=0,index_col=0)\n",
    "unique_links = unique_wikis['wikilink'].unique().tolist()\n",
    "titlelist = [x.replace(\" \",\"_\").replace(\"https://\",\"http://\").replace(\"http://en.wikipedia.org/wiki/\",\"\") for x in unique_links]\n",
    "\n",
    "pageinfo,pagefails = get_wiki_volume_info(mwsite,titlelist)\n",
    "wikiinfo = pd.DataFrame(pageinfo)\n",
    "print(wikiinfo.head(n=2))\n",
    "\n",
    "wikiinfo.to_csv(os.path.join(datapath,'gene_wiki_vol_info.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9074\n"
     ]
    }
   ],
   "source": [
    "#### Filter out Wikipedia articles which are greater than 10,000 characters in length\n",
    "shorter_articles = wikiinfo.loc[wikiinfo['page_length']<10000]\n",
    "print(len(shorter_articles))\n",
    "\n",
    "#### Use filtered gene list to get number of pubmed ids associated with each gene and save pmids if # of records >30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
